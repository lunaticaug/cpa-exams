"""
Process 2 : ê²Œì‹œë¬¼ë³„ ì²¨ë¶€íŒŒì¼ URL ë¦¬ìŠ¤íŠ¸(csv) ìƒì„±
-------------------------------------------------
ì…ë ¥ : 01_post_urls_cache.csv  (Process 1 ê²°ê³¼)
ì¶œë ¥ : 02_file_urls_cache.csv  (ëª¨ë“  ì²¨ë¶€íŒŒì¼ ì •ë³´)
       02_pdf_urls_cache.csv   (PDFë§Œ ë”°ë¡œ, ì„ íƒ ì‚¬í•­)

* ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ í´ë”ì— CSVë¥¼ ì½ê³  ì”ë‹ˆë‹¤.
"""

import os
import re
import csv
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs
from typing import Tuple, List

# --------------------------- ìœ í‹¸ --------------------------- #

def load_post_urls(csv_path: str) -> List[str]:
    """01_post_urls_cache.csv ì—ì„œ ê²Œì‹œë¬¼ URL ë¦¬ìŠ¤íŠ¸ë¥¼ ì½ì–´ ë°˜í™˜"""
    with open(csv_path, newline='', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        return [row["url"] for row in reader]


def extract_meta(soup: BeautifulSoup) -> Tuple[str, str, str]:
    """
    ê²Œì‹œë¬¼ ì œëª©ì—ì„œ
      Â· title : ì „ì²´ ì œëª© ë¬¸ìì—´
      Â· year  : 20XX (ì—†ìœ¼ë©´ 'unknown')
      Â· phase : 1ì°¨ / 2ì°¨ (ë‹¤ì–‘í•œ í‘œê¸° í—ˆìš©, ì—†ìœ¼ë©´ 'unknown')
    ë¥¼ ì¶”ì¶œí•´ (year, phase, title) ë¡œ ë°˜í™˜
    """
    # ì œëª© ìœ„ì¹˜: <div class="board-title"><div class="subject"><h3>...</h3></div></div>
    title_tag = soup.select_one("div.board-title div.subject h3")
    title = title_tag.get_text(strip=True) if title_tag else ""

    # ì—°ë„: 2000~2099
    m_year = re.search(r"(20\d{2})", title)
    year = m_year.group(1) if m_year else "unknown"

    # ì‹œí—˜êµ¬ë¶„: 1ì°¨Â·2ì°¨ / ì œ1ì°¨Â·ì œ2ì°¨ / ì œ1ì‹œí—˜Â·ì œ2ì‹œí—˜
    m_phase = re.search(r"(?:ì œ)?\s*(1ì°¨|2ì°¨|1ì‹œí—˜|2ì‹œí—˜)", title)
    if m_phase:
        raw = m_phase.group(1)
        phase = "1ì°¨" if raw.startswith("1") else "2ì°¨"
    else:
        phase = "unknown"

    return year, phase, title


def build_file_url(atch_id: str, bbs_id: str, file_sn: int) -> str:
    """ì²¨ë¶€íŒŒì¼ download URL ê·œì¹™í™”"""
    return (
        "https://cpa.fss.or.kr/cpa/cmmn/file/fileDown.do"
        f"?menuNo=&atchFileId={atch_id}&fileSn={file_sn}&bbsId={bbs_id}"
    )

# ------------------------- ë©”ì¸ ë¡œì§ ------------------------ #

def get_file_urls(post_urls: List[str], max_filesn: int = 10):
    session = requests.Session()
    session.headers.update({"User-Agent": "Mozilla/5.0"})

    all_records = []
    pdf_urls = []

    for post_url in post_urls:
        try:
            resp = session.get(post_url, timeout=10)
            resp.raise_for_status()
        except Exception as e:
            print(f"   â›” ìš”ì²­ ì‹¤íŒ¨: {e} â€” {post_url}")
            continue

        soup = BeautifulSoup(resp.text, "lxml")

        # (1) ë©”íƒ€ì •ë³´
        year, phase, title = extract_meta(soup)
        print(f"â³ [{title or 'ì œëª© ì—†ìŒ'}] â€” {post_url}")

        # (2) ì²« ë²ˆì§¸ ì²¨ë¶€íŒŒì¼ ë§í¬ì—ì„œ atchFileId / bbsId í™•ë³´
        a_file = soup.find("a", href=lambda h: h and "fileDown.do" in h)
        if not a_file:
            print("   âŒ ì²¨ë¶€íŒŒì¼ ì—†ìŒ ğŸ˜“")
            continue
        qs = parse_qs(urlparse(a_file["href"]).query)
        atch_id = qs.get("atchFileId", [""])[0]
        bbs_id = qs.get("bbsId", [""])[0]

        # (3) fileSn 1 ~ max_filesn ìˆœíšŒ
        for file_sn in range(1, max_filesn + 1):
            f_url = build_file_url(atch_id, bbs_id, file_sn)
            head = session.head(f_url, allow_redirects=True, timeout=5)
            exists = head.status_code == 200
            is_pdf = False
            if exists:
                cd = head.headers.get("Content-Disposition", "").lower()
                ct = head.headers.get("Content-Type", "").lower()
                if "pdf" in cd or "pdf" in ct:
                    is_pdf = True
                    pdf_urls.append(f_url)

            all_records.append(
                {
                    "post_url": post_url,
                    "title": title,
                    "year": year,
                    "phase": phase,
                    "atchFileId": atch_id,
                    "bbsId": bbs_id,
                    "file_sn": file_sn,
                    "url": f_url,
                    "exists": exists,
                    "is_pdf": is_pdf,
                }
            )

    return all_records, pdf_urls

# ------------------------- ì‹¤í–‰ë¶€ -------------------------- #
if __name__ == "__main__":
    script_dir = os.path.dirname(os.path.abspath(__file__))

    posts_csv = os.path.join(script_dir, "01_post_urls_cache.csv")
    files_csv = os.path.join(script_dir, "02_file_urls_cache.csv")
    pdfs_csv = os.path.join(script_dir, "02_pdf_urls_cache.csv")

    post_urls = load_post_urls(posts_csv)
    records, pdf_urls = get_file_urls(post_urls, max_filesn=10)

    # (A) ì „ì²´ ì²¨ë¶€íŒŒì¼ ì •ë³´ ì €ì¥
    with open(files_csv, "w", newline="", encoding="utf-8") as f:
        fieldnames = [
            "post_url",
            "title",
            "year",
            "phase",
            "atchFileId",
            "bbsId",
            "file_sn",
            "url",
            "exists",
            "is_pdf",
        ]
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(records)

    # (B) PDF ì „ìš© ë¦¬ìŠ¤íŠ¸(ì„ íƒ)
    with open(pdfs_csv, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["url"])
        for u in pdf_urls:
            writer.writerow([u])

    print(
        f"\nì™„ë£Œâœ…: {len(records)}ê°œ íŒŒì¼ â†’ '{files_csv}', "
        f"{len(pdf_urls)}ê°œ PDF â†’ '{pdfs_csv}' ğŸ’¾ì €ì¥"
    )
